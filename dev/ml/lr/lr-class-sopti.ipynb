{
 "metadata": {
  "name": "lr-class-sopti"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math \n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from scipy import optimize"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = pd.read_csv('ex2data1.txt', header=None)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = data[[0,1]].values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y = data[2].values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class LogisticRegression(object):\n",
      "    \n",
      "    def __init__(self, reg_lambda=0, opti_method='TNC'):\n",
      "        self.reg_lambda = reg_lambda\n",
      "        self.method = opti_method\n",
      "    \n",
      "    def sigmoid(self, z):\n",
      "        return 1 / (1 + np.exp(-z))\n",
      "    \n",
      "    def hypothesis(self, X, theta):\n",
      "        return self.sigmoid(np.dot(X, theta))\n",
      "    \n",
      "    def function(self, theta, X, y):\n",
      "        m = X.shape[0]\n",
      "        h = self.hypothesis(X, theta)\n",
      "        \n",
      "        costPos = np.dot(-y.T, np.log(h))\n",
      "        costNeg = np.dot((1 - y).T, np.log(1 - h))\n",
      "        J = (costPos - costNeg) / m\n",
      "        \n",
      "        if self.reg_lambda != 0:\n",
      "            theta_filtered = np.append([0], theta[1:])\n",
      "            J = J + (self.reg_lambda / (2*m)) * np.dot(theta_filtered.T, theta_filtered)\n",
      "        \n",
      "        return J\n",
      "    \n",
      "    def function_prime(self, theta, X, y):\n",
      "        m = X.shape[0]\n",
      "        h = self.hypothesis(X, theta)\n",
      "        grad = np.dot(X.T, h - y) / m\n",
      "        \n",
      "        if self.reg_lambda != 0:\n",
      "            theta_filtered = np.append([0], theta[1:])\n",
      "            grad = grad + np.dot(self.reg_lambda / m, theta_filtered)\n",
      "        return grad\n",
      "    \n",
      "    def minibatch(self, X, y):\n",
      "        m = X.shape[0]\n",
      "        batch_size = 0.1\n",
      "        batch_size = int(math.floor(m * batch_size))\n",
      "        while True:\n",
      "            indices = np.random.choice(np.arange(m), batch_size, replace=False)\n",
      "            yield X[indices], y[indices]\n",
      "    \n",
      "    def fit(self, X, y):\n",
      "        m, n = X.shape\n",
      "        X = np.append(np.ones(m).reshape(m,1), X, axis=1)\n",
      "        thetas0 = np.zeros(n + 1)\n",
      "        \n",
      "        thetas = thetas0\n",
      "        _res = None\n",
      "        prev = 1000\n",
      "        tol = 0.00001\n",
      "        i = 0\n",
      "        for _X, _y in self.minibatch(X, y):\n",
      "            options = {'maxiter': 1}\n",
      "            _res = optimize.minimize(self.function, thetas0, jac=self.function_prime, method=self.method, args=(_X, _y), options=options)\n",
      "            if np.abs(prev - _res.fun) < tol or i >= 100000:\n",
      "                break\n",
      "            i += 1\n",
      "            #print i, _res.fun\n",
      "        print tol, i \n",
      "        self.thetas = _res.x\n",
      "        \n",
      "    def predict_proba(self, X):\n",
      "        m, n = X.shape\n",
      "        X = np.append(np.ones(m).reshape(m,1), X, axis=1)\n",
      "        return self.hypothesis(X, self.thetas)\n",
      "    \n",
      "    def predict(self, X):\n",
      "        return (self.predict_proba(X) >= 0.5).astype(int)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lr = LogisticRegression(opti_method='Newton-CG')\n",
      "lr.fit(X, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1e-05 100000\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(lr.predict(X) == y).mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "0.5"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit lr.fit(X, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10 loops, best of 3: 35.5 ms per loop\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lr.thetas"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 36,
       "text": [
        "array([-1.45997214,  0.55469685,  0.11127421])"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}