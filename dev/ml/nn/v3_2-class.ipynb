{
 "metadata": {
  "name": "v3_2-class"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math\n",
      "import numpy as np\n",
      "from scipy import optimize"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Logistic Layer"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def softmax(x):\n",
      "    e = np.exp(x - np.max(x))  # prevent overflow\n",
      "    if e.ndim == 1:\n",
      "        return e / np.sum(e, axis=0)\n",
      "    else:  \n",
      "        return e / np.array([np.sum(e, axis=1)]).T  # ndim = 2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Layer(object):\n",
      "    def __init__(self, n_in=None, n_out=None, W=None, random_state=None, activation=None):\n",
      "        if random_state is None:\n",
      "            rnd = np.random.RandomState()\n",
      "        else:\n",
      "            rnd = random_state\n",
      "        \n",
      "        if W is None:\n",
      "            self.W = rnd.uniform(size=(n_out, n_in + 1))\n",
      "        else:\n",
      "            self.W = W\n",
      "        \n",
      "        self.activation = activation\n",
      "        \n",
      "    def output(self, input):\n",
      "        data = np.insert(input, 0, 1, axis=1)\n",
      "        linear_output = np.dot(data, self.W.T)\n",
      "        return self.activation(linear_output)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class LogisticLayer(Layer):\n",
      "    def __init__(self, n_in=None, n_out=None, W=None, random_state=None):\n",
      "        Layer.__init__(self, n_in, n_out, W, random_state, activation=softmax)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Hidden/Sigmoid Layer"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sigmoid(z):\n",
      "    return np.divide(1., (1 + np.exp(-z)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class SigmoidLayer(Layer):\n",
      "    def __init__(self, n_in=None, n_out=None, W=None, random_state=None):\n",
      "        Layer.__init__(self, n_in, n_out, W, random_state, activation=sigmoid)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Network"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_info(layers):\n",
      "    return [(layers[i + 1], layers[i] + 1) for i in range(len(layers) - 1)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def rand_init(weights_info, random_state):\n",
      "    w_size = 0\n",
      "    for layer_info in weights_info:\n",
      "        w_size += layer_info[0] * layer_info[1]\n",
      "    return random_state.uniform(size=w_size)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_layers(weights, weights_info):\n",
      "    layers = []\n",
      "    # Unpack the weights and assing them\n",
      "    start_pos = 0\n",
      "    for w_info in weights_info:\n",
      "        end_pos = start_pos + w_info[0] * (w_info[1])\n",
      "        weight = weights[start_pos:end_pos].reshape((w_info[0], w_info[1]))\n",
      "        layers.append(SigmoidLayer(W=weight))\n",
      "        start_pos = end_pos\n",
      "    return layers"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def forward(layers, X):\n",
      "    output = layers[0].output(X)\n",
      "    for layer in layers[1:]:\n",
      "        output = layer.output(output)\n",
      "    return output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Cost function and gradient"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def unpack_weigths(weights, weights_meta):\n",
      "    start_pos = 0\n",
      "    for layer in weights_meta:\n",
      "        end_pos = start_pos + layer[0] * (layer[1])\n",
      "        yield weights[start_pos:end_pos].reshape((layer[0], layer[1]))\n",
      "        start_pos = end_pos"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def cost(weights, X, y, weights_meta, num_labels):\n",
      "    # Forward\n",
      "    act_prev = np.insert(X, 0, 1, axis=1)\n",
      "    for weight in unpack_weigths(weights, weights_meta):\n",
      "        z = np.dot(act_prev, weight.T)\n",
      "        activation = sigmoid(z)\n",
      "        act_prev = np.insert(activation, 0, 1, axis=1)\n",
      "    \n",
      "    Y = np.eye(num_labels)[y]\n",
      "    h = activation\n",
      "    costPositive = -Y * np.log(h)\n",
      "    costNegative = (1 - Y) * np.log(1 - h)\n",
      "    J = np.sum(costPositive - costNegative) / X.shape[0]\n",
      "    \n",
      "    return J"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def unpack_weigths_inv(weights, weights_meta):\n",
      "    end_pos = len(weights)\n",
      "    for layer in reversed(weights_meta):\n",
      "        start_pos = end_pos - layer[0] * (layer[1])\n",
      "        yield weights[start_pos:end_pos].reshape((layer[0], layer[1]))\n",
      "        end_pos = start_pos"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def cost_prime(weights, X, y, weights_meta, num_labels):\n",
      "    Y = np.eye(num_labels)[y]\n",
      "    Deltas = [np.zeros(shape) for shape in weights_meta]\n",
      "    \n",
      "    data = np.insert(X, 0, 1, axis=1)\n",
      "    for i, row in enumerate(data):\n",
      "        # Forward\n",
      "        #row = np.array([row])\n",
      "        act_prev = row\n",
      "        activations = (act_prev, )\n",
      "        for weight in unpack_weigths(weights, weights_meta):\n",
      "            z = np.dot(act_prev, weight.T)\n",
      "            activation = sigmoid(z)\n",
      "            act_prev = np.append(1, activation)\n",
      "            activations = activations + (act_prev, )\n",
      "        \n",
      "        # Backprop\n",
      "        prev_delta = activations[-1][1:] - Y[i, :].T  # last delta\n",
      "        deltas = (prev_delta, )  # deltas[0] == delta2\n",
      "        for act, weight in zip(reversed(activations[1:-1]), unpack_weigths_inv(weights, weights_meta)):\n",
      "            delta = np.dot(weight.T, prev_delta)[1:] * (act[1:] * (1 - act[1:])).T\n",
      "            deltas = (delta, ) + deltas\n",
      "            prev_delta = delta\n",
      "        \n",
      "        # Accumulate errors\n",
      "        for delta, act, i in zip(deltas, activations[:-1], range(len(Deltas))):\n",
      "            Deltas[i] = Deltas[i] + np.dot(delta[np.newaxis].T, act[np.newaxis])\n",
      "    for i in range(len(Deltas)):\n",
      "        Deltas[i] = Deltas[i] / X.shape[0]\n",
      "    return np.concatenate(tuple([D.reshape(-1) for D in Deltas]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Optimization"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class MinibatchOpti(object):\n",
      "    \n",
      "    @staticmethod\n",
      "    def minibatches(X, y=None, batch_size=50, random_state=None):\n",
      "        if random_state is None:\n",
      "            rnd = np.random.RandomState()\n",
      "        elif isinstance(random_state, int):\n",
      "            rnd = np.random.RandomState(random_state)\n",
      "        else:\n",
      "           rnd = random_state\n",
      "\n",
      "        m = X.shape[0]\n",
      "        batch_size = batch_size if batch_size >= 1 else int(math.floor(m * batch_size))\n",
      "        max_batchs = int(math.floor(m / batch_size))\n",
      "        \n",
      "        while True:\n",
      "            random_indices = rnd.choice(np.arange(m), m, replace=False)\n",
      "            for i in range(max_batchs):\n",
      "                batch_indices = np.arange(i * batch_size, (i + 1) * batch_size)\n",
      "                indices = random_indices[batch_indices]\n",
      "                if y is None:\n",
      "                    yield X[indices]\n",
      "                else:\n",
      "                    yield X[indices], y[indices]\n",
      "    @staticmethod\n",
      "    def GD(fun, weights, jac, X, y, options, args=()):\n",
      "        weights -= options['learning_rate'] * jac(weights, X, y, *args)\n",
      "        options['learning_rate'] = options['learning_rate'] * options['learning_rate_decay']\n",
      "    \n",
      "    @staticmethod\n",
      "    def GD_momentum(fun, weights, jac, X, y, options, args=()):\n",
      "        bigjump = options['momentum'] * options['step']\n",
      "        weights -= bigjump\n",
      "        correction = options['learning_rate'] * jac(weights, X, y, *args)\n",
      "        weights -= correction\n",
      "        options['step'] = bigjump + correction\n",
      "        options['learning_rate'] = options['learning_rate'] * options['learning_rate_decay']\n",
      "        options['momentum'] = options['momemtum_decay'] * options['momentum']\n",
      "        \n",
      "    @staticmethod\n",
      "    def RMSPROP(fun, weights, jac, X, y, options, args=()):\n",
      "        gradient = jac(weights, X, y, *args)\n",
      "        options['moving_mean_squared'] = options['decay'] * options['moving_mean_squared'] \\\n",
      "                                         + (1 - options['decay']) * gradient ** 2\n",
      "        weights -= gradient / np.sqrt(options['moving_mean_squared'] + 1e-8)\n",
      "        \n",
      "    @staticmethod\n",
      "    def CG(fun, weights, jac, X, y, options, args=()):\n",
      "        ans = optimize.minimize(fun, weights, jac=jac, method='CG', args=(X, y) + args, options={'maxiter': options['mb_maxiter']})\n",
      "        weights[:] = ans.x\n",
      "        \n",
      "    @staticmethod\n",
      "    def LBFGSB(fun, weights, jac, X, y, options, args=()):\n",
      "        ans = optimize.minimize(fun, weights, jac=jac, method='L-BFGS-B', args=(X, y) + args, options={'maxiter': options['mb_maxiter']})\n",
      "        weights[:] = ans.x\n",
      "    \n",
      "    @staticmethod\n",
      "    def minimize(fun, weights, jac, X, y, method, batch_size=50, tol=1e-6, maxiter=100, args=None, \n",
      "                 verbose=False, options=None, random_state=None, callback=None):\n",
      "        if method == 'GD':\n",
      "            assert 'learning_rate' in options, 'GD needs a learning rate'\n",
      "            if 'learning_rate_decay' not in options:\n",
      "                options['learning_rate_decay'] = 1\n",
      "            if 'momentum' in options:\n",
      "                if 'momemtum_decay' not in options:\n",
      "                    options['momemtum_decay'] = 1\n",
      "                options['step'] = 0\n",
      "                update = MinibatchOpti.GD_momentum\n",
      "            else:\n",
      "                update = MinibatchOpti.GD\n",
      "        elif method == 'RMSPROP':\n",
      "            options['moving_mean_squared'] = 1\n",
      "            update = MinibatchOpti.RMSPROP\n",
      "        elif method == 'CG':\n",
      "            update = MinibatchOpti.CG\n",
      "        elif method == 'L-BFGS-B':\n",
      "            update = MinibatchOpti.LBFGSB\n",
      "        else:\n",
      "            raise Exception('Optimization method not found')\n",
      "\n",
      "        i = 1\n",
      "        prev_cost = 1e8\n",
      "        for _X, _y in MinibatchOpti.minibatches(X, y, batch_size, random_state=random_state):\n",
      "            update(fun, weights, jac, _X, _y, options, args=args)\n",
      "            new_cost = fun(weights, X, y, *args)\n",
      "            diff = new_cost - prev_cost\n",
      "            if np.abs(diff) < tol:\n",
      "                if verbose >= 1:\n",
      "                    print 'Minimum tolerance reached in %i iterations' % i\n",
      "                break\n",
      "            if i >= maxiter:\n",
      "                if verbose >= 1 :\n",
      "                    print 'Maximum number of iterations reached'\n",
      "                break\n",
      "            if verbose >= 2:\n",
      "                print i, new_cost    \n",
      "            if callback is not None:\n",
      "                stop = callback(i, weights)\n",
      "                if stop == True:\n",
      "                    break\n",
      "            prev_cost = new_cost\n",
      "            i = i + 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Class"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NN(object):\n",
      "    \n",
      "    def __init__(self, hidden_layers, coef0=None, random_state=None,\n",
      "                 opti_method='GD', batch_size=50, maxiter=100, tol=1e-6, verbose=1, \n",
      "                 opti_options=None, callback=None):\n",
      "        self.hidden_layers = hidden_layers\n",
      "        self.coef_ = None if coef0 is None else np.copy(coef0)\n",
      "        \n",
      "        if random_state is None:\n",
      "            self.rnd = np.random.RandomState()\n",
      "        elif isinstance(random_state, int):\n",
      "            self.rnd = np.random.RandomState(random_state)\n",
      "        else:\n",
      "            self.rnd = random_state\n",
      "        \n",
      "        self.opti_method = opti_method\n",
      "        self.batch_size = batch_size\n",
      "        self.verbose = verbose\n",
      "        self.maxiter = maxiter\n",
      "        self.tol = tol\n",
      "        self.opti_options = {} if opti_options is None else opti_options\n",
      "        self.callback = callback\n",
      "    \n",
      "    def predict_proba(self, X):\n",
      "        output = self.layers[0].output(X)\n",
      "        for layer in self.layers[1:]:\n",
      "            output = layer.output(output)\n",
      "        return output\n",
      "   \n",
      "    def predict(self, X):\n",
      "        return self.predict_proba(X).argmax(1)\n",
      "    \n",
      "    def fit(self, X, y):\n",
      "        layer_sizes = list(self.hidden_layers)  # Copy\n",
      "        layer_sizes.insert(0, X.shape[1])\n",
      "        layer_sizes.insert(len(layer_sizes), len(np.unique(y)))\n",
      "        self.weights_info = create_info(layer_sizes)\n",
      "        self.opti_options = self.opti_options.copy()\n",
      "        \n",
      "        if self.coef_ is None:\n",
      "            self.coef_ = rand_init(self.weights_info, self.rnd)\n",
      "\n",
      "        # Unpack the weights and assign them to the layers\n",
      "        self.layers = []\n",
      "        start_pos = 0\n",
      "        for w_info in self.weights_info:\n",
      "            end_pos = start_pos + w_info[0] * (w_info[1])\n",
      "            weight = self.coef_[start_pos:end_pos].reshape((w_info[0], w_info[1]))\n",
      "            self.layers.append(SigmoidLayer(W=weight))\n",
      "            start_pos = end_pos\n",
      "        \n",
      "        args = (self.weights_info, len(np.unique(y)))\n",
      "        MinibatchOpti.minimize(cost, self.coef_, cost_prime, X, y, method=self.opti_method,\n",
      "                               random_state=self.rnd, batch_size=self.batch_size, maxiter=self.maxiter, \n",
      "                               tol=self.tol, args=args, verbose=self.verbose, options=self.opti_options,\n",
      "                               callback=self.callback)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "options = {}\n",
      "options['learning_rate'] = 0.3\n",
      "options['learning_rate_decay'] = 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "options['momemtum'] = 0.9\n",
      "options['decay'] = 0.9\n",
      "options['mb_maxiter'] = 2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def validate(epoch, weights):\n",
      "    if (nn.predict(X) == y).mean() > 0.50:\n",
      "        return True"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cPickle, gzip, numpy\n",
      "f = gzip.open('mnist.pkl.gz', 'rb')\n",
      "train_set, valid_set, test_set = cPickle.load(f)\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, y_train = train_set[0], train_set[1]\n",
      "X_valid, y_valid = valid_set[0], valid_set[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nn = NN([25], verbose=2, opti_method='CG', maxiter=10, opti_options=options, random_state=1234)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nn.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 10.7569331658\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5.24745316828\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4.17050983186\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3.42591428066\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3.37097953817\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3.36979548844\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3.3515728943\n",
        "8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3.32369309715\n",
        "9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3.45300397936\n",
        "Maximum number of iterations reached"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(nn.predict(X_train) == y_train).mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 36,
       "text": [
        "0.099360000000000004"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import datasets\n",
      "iris = datasets.load_iris()\n",
      "X = iris.data\n",
      "y = iris.target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 52
    }
   ],
   "metadata": {}
  }
 ]
}