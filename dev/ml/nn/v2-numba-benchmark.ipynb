{
 "metadata": {
  "name": "v2-numba-benchmark"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Version 2 is the an update to version 1 to make it more general and easy to run becnhmarks. 1 Hidden Layer."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import math\n",
      "from scipy import optimize\n",
      "from scipy.io import loadmat\n",
      "from __future__ import division\n",
      "import sklearn.datasets as datasets\n",
      "from sklearn import cross_validation\n",
      "from numbapro import autojit, jit"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "iris = datasets.load_iris()\n",
      "X = iris.data\n",
      "y = iris.target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = loadmat('ex3data1.mat')\n",
      "X, y = data['X'], data['y']\n",
      "y = y.reshape(X.shape[0], )\n",
      "y = y - 1  # Fix notation"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.4, random_state=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "input_layer_size = X.shape[1]\n",
      "hidden_layer_size = 25\n",
      "num_labels = len(np.unique(y))\n",
      "epsilon_init = 0.12"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "arr_large = np.random.random(1000000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Sigmoid"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def hard_sigmoid(x):\n",
      "    z = (x * 0.2) + 0.5\n",
      "    z[z > 1] = 1\n",
      "    z[z < 0] = 0\n",
      "    return z"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit hard_sigmoid(arr_large)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "100 loops, best of 3: 8.2 ms per loop\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numba"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numba import float64"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "numba.typeof(np.random.rand(10, 10))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "float64[:, :]"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "@jit(float64[:](float64[:]))\n",
      "def hard_sigmoid_numba(x):\n",
      "    z = (x * 0.2) + 0.5\n",
      "    z[z > 1] = 1\n",
      "    z[z < 0] = 0\n",
      "    return z"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit hard_sigmoid_numba(arr_large)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "100 loops, best of 3: 6.96 ms per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sigmoid_prime(z):\n",
      "    sig = hard_sigmoid(z)\n",
      "    return sig * (1 - sig)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit sigmoid_prime(arr_large)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "100 loops, best of 3: 15.2 ms per loop\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sigmoid_prime_numba = autojit(sigmoid_prime)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "@autojit\n",
      "def sigmoid_prime_numba(z):\n",
      "    sig = hard_sigmoid_numba(z)\n",
      "    return sig * (1 - sig)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit sigmoid_prime_numba(arr_large)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 loops, best of 3: 9.74 ms per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Sumsqr"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sumsqr(a):\n",
      "    return np.sum(a ** 2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit sumsqr(arr_large)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "100 loops, best of 3: 4.3 ms per loop\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sumsqr_numba = autojit(sumsqr)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit sumsqr_numba(arr_large)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 loops, best of 3: 18.9 ms per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Thetas methods"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def rand_init(l_in, l_out):\n",
      "    return np.random.rand(l_out, l_in + 1) * 2 * epsilon_init - epsilon_init"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "theta1_0 = rand_init(input_layer_size, hidden_layer_size)\n",
      "theta2_0 = rand_init(hidden_layer_size, num_labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def pack_thetas(t1, t2):\n",
      "    return np.concatenate((t1.reshape(-1), t2.reshape(-1)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def unpack_thetas(thetas, input_layer_size, hidden_layer_size, num_labels):\n",
      "    t1_start = 0\n",
      "    t1_end = hidden_layer_size * (input_layer_size + 1)\n",
      "    t1 = thetas[t1_start:t1_end].reshape((hidden_layer_size, input_layer_size + 1))\n",
      "    t2 = thetas[t1_end:].reshape((num_labels, hidden_layer_size + 1))\n",
      "    return t1, t2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Forward"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def _forward(X, t1, t2):\n",
      "    m = X.shape[0]\n",
      "    ones = None\n",
      "    if len(X.shape) == 1:\n",
      "        ones = np.array(1).reshape(1,)\n",
      "    else:\n",
      "        ones = np.ones(m).reshape(m,1)\n",
      "    \n",
      "    # Input layer\n",
      "    a1 = np.hstack((ones, X))\n",
      "    \n",
      "    # Hidden Layer\n",
      "    z2 = np.dot(t1, a1.T)\n",
      "    #a2 = self.activation_func(z2)\n",
      "    a2 = hard_sigmoid(z2)\n",
      "    a2 = np.hstack((ones, a2.T))\n",
      "    \n",
      "    # Output layer\n",
      "    z3 = np.dot(t2, a2.T)\n",
      "    #a3 = self.activation_func(z3)\n",
      "    a3 = hard_sigmoid(z3)\n",
      "    return a1, z2, a2, z3, a3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "% timeit _forward(X, theta1_0, theta2_0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "100 loops, best of 3: 14.7 ms per loop\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "@jit(float64[:, :](float64[:, :]))\n",
      "def hard_sigmoid_numba_matrix(x):\n",
      "    z = (x * 0.2) + 0.5\n",
      "    z[z > 1] = 1\n",
      "    z[z < 0] = 0\n",
      "    return z"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "@jit(float64[:, :](float64[:, :], float64[:, :], float64[:, :]))\n",
      "def _forward_numba(X, t1, t2):\n",
      "    m = X.shape[0]\n",
      "    ones = None\n",
      "    if len(X.shape) == 1:\n",
      "        ones = np.array(1).reshape(1,)\n",
      "    else:\n",
      "        ones = np.ones(m).reshape(m,1)\n",
      "    \n",
      "    # Input layer\n",
      "    a1 = np.hstack((ones, X))\n",
      "    \n",
      "    # Hidden Layer\n",
      "    z2 = np.dot(t1, a1.T)\n",
      "    #a2 = self.activation_func(z2)\n",
      "    a2 = hard_sigmoid_numba_matrix(z2)\n",
      "    a2 = np.hstack((ones, a2.T))\n",
      "    \n",
      "    # Output layer\n",
      "    z3 = np.dot(t2, a2.T)\n",
      "    #a3 = self.activation_func(z3)\n",
      "    a3 = hard_sigmoid_numba_matrix(z3)\n",
      "    return a1, z2, a2, z3, a3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "% timeit _forward_numba(X, theta1_0, theta2_0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "100 loops, best of 3: 13.8 ms per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Minibatch"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def minibatch(X, y, batch_size):\n",
      "    m = X.shape[0]\n",
      "    batch_size = int(math.floor(m * batch_size))\n",
      "    max_batchs = int(math.floor(m / batch_size))\n",
      "    \n",
      "    indices = np.random.choice(np.arange(m), m, replace=False)\n",
      "    X, y = X[indices], y[indices]\n",
      "    while True:\n",
      "        for i in range(max_batchs):\n",
      "            indices =  np.arange(i*batch_size, (i + 1) * batch_size)\n",
      "            yield X[indices], y[indices]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit minibatch(X, y, 0.1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1000000 loops, best of 3: 502 ns per loop\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "@jit(float64[:, :](float64[:, :], float64[:], float64))\n",
      "def minibatch_numba(X, y, batch_size):\n",
      "    m = X.shape[0]\n",
      "    batch_size = int(math.floor(m * batch_size))\n",
      "    max_batchs = int(math.floor(m / batch_size))\n",
      "    \n",
      "    indices = np.random.choice(np.arange(m), m, replace=False)\n",
      "    X, y = X[indices], y[indices]\n",
      "    while True:\n",
      "        for i in range(max_batchs):\n",
      "            indices =  np.arange(i*batch_size, (i + 1) * batch_size)\n",
      "            return X[indices]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit minibatch_numba(X, y, 0.1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10 loops, best of 3: 116 ms per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Function and derivative"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "theta1_0 = rand_init(input_layer_size, hidden_layer_size)\n",
      "theta2_0 = rand_init(hidden_layer_size, num_labels)\n",
      "thetas0 = pack_thetas(theta1_0, theta2_0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def function(thetas, input_layer_size, hidden_layer_size, num_labels, X, y, reg_lambda):\n",
      "    t1, t2 = unpack_thetas(thetas, input_layer_size, hidden_layer_size, num_labels)\n",
      "    m = X.shape[0]\n",
      "    Y = np.eye(num_labels)[y]\n",
      "    \n",
      "    _, _, _, _, h = _forward(X, t1, t2)\n",
      "    costPositive = -Y * np.log(h).T\n",
      "    costNegative = (1 - Y) * np.log(1 - h).T\n",
      "    cost = costPositive - costNegative\n",
      "    J = np.sum(cost) / m\n",
      "    \n",
      "    if reg_lambda != 0:\n",
      "        t1f = t1[:, 1:]\n",
      "        t2f = t2[:, 1:]\n",
      "        reg = (reg_lambda / (2 * m)) * (sumsqr(t1f) + sumsqr(t2f))\n",
      "        J = J + reg\n",
      "    return J"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#@jit(float64(float64[:], float64, float64, float64, float64[:, :], float64[:], float64))\n",
      "def function_numba(thetas, input_layer_size, hidden_layer_size, num_labels, X, y, reg_lambda):\n",
      "    t1, t2 = unpack_thetas(thetas, input_layer_size, hidden_layer_size, num_labels)\n",
      "    m = X.shape[0]\n",
      "    Y = np.eye(num_labels)[y]\n",
      "    \n",
      "    _, _, _, _, h = _forward_numba(X, t1, t2)\n",
      "    costPositive = -Y * np.log(h).T\n",
      "    costNegative = (1 - Y) * np.log(1 - h).T\n",
      "    cost = costPositive - costNegative\n",
      "    J = np.sum(cost) / m\n",
      "    \n",
      "    if reg_lambda != 0:\n",
      "        t1f = t1[:, 1:]\n",
      "        t2f = t2[:, 1:]\n",
      "        reg = (reg_lambda / (2 * m)) * (sumsqr(t1f) + sumsqr(t2f))\n",
      "        J = J + reg\n",
      "    return J"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function_numba(thetas0, input_layer_size, hidden_layer_size, num_labels, X, y, 0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 39,
       "text": [
        "6.7380407976938859"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def function_prime(thetas, input_layer_size, hidden_layer_size, num_labels, X, y, reg_lambda):\n",
      "    t1, t2 = unpack_thetas(thetas, input_layer_size, hidden_layer_size, num_labels)\n",
      "    \n",
      "    m = X.shape[0]\n",
      "    t1f = t1[:, 1:]\n",
      "    t2f = t2[:, 1:]\n",
      "    Y = np.eye(num_labels)[y]\n",
      "    \n",
      "    Delta1, Delta2 = 0, 0\n",
      "    for i, row in enumerate(X):\n",
      "        a1, z2, a2, z3, a3 = _forward(row, t1, t2)\n",
      "        \n",
      "        # Backprop\n",
      "        d3 = a3 - Y[i, :].T\n",
      "        d2 = np.dot(t2f.T, d3) * hard_sigmoid(z2)\n",
      "        \n",
      "        Delta2 += np.dot(d3[np.newaxis].T, a2[np.newaxis])\n",
      "        Delta1 += np.dot(d2[np.newaxis].T, a1[np.newaxis])\n",
      "        \n",
      "    Theta1_grad = (1 / m) * Delta1\n",
      "    Theta2_grad = (1 / m) * Delta2\n",
      "    \n",
      "    if reg_lambda != 0:\n",
      "        Theta1_grad[:, 1:] = Theta1_grad[:, 1:] + (reg_lambda / m) * t1f\n",
      "        Theta2_grad[:, 1:] = Theta2_grad[:, 1:] + (reg_lambda / m) * t2f\n",
      "    \n",
      "    return pack_thetas(Theta1_grad, Theta2_grad)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def function_prime_numba(thetas, input_layer_size, hidden_layer_size, num_labels, X, y, reg_lambda):\n",
      "    t1, t2 = unpack_thetas(thetas, input_layer_size, hidden_layer_size, num_labels)\n",
      "    \n",
      "    m = X.shape[0]\n",
      "    t1f = t1[:, 1:]\n",
      "    t2f = t2[:, 1:]\n",
      "    Y = np.eye(num_labels)[y]\n",
      "    \n",
      "    Delta1, Delta2 = 0, 0\n",
      "    for i, row in enumerate(X):\n",
      "        a1, z2, a2, z3, a3 = _forward(row, t1, t2)\n",
      "        \n",
      "        # Backprop\n",
      "        d3 = a3 - Y[i, :].T\n",
      "        d2 = np.dot(t2f.T, d3) * hard_sigmoid_numba(z2)\n",
      "        \n",
      "        Delta2 += np.dot(d3[np.newaxis].T, a2[np.newaxis])\n",
      "        Delta1 += np.dot(d2[np.newaxis].T, a1[np.newaxis])\n",
      "        \n",
      "    Theta1_grad = (1 / m) * Delta1\n",
      "    Theta2_grad = (1 / m) * Delta2\n",
      "    \n",
      "    if reg_lambda != 0:\n",
      "        Theta1_grad[:, 1:] = Theta1_grad[:, 1:] + (reg_lambda / m) * t1f\n",
      "        Theta2_grad[:, 1:] = Theta2_grad[:, 1:] + (reg_lambda / m) * t2f\n",
      "    \n",
      "    return pack_thetas(Theta1_grad, Theta2_grad)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "SGD"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def fit(X, y):\n",
      "    batch_size = 0.1\n",
      "    reg_lambda = 0\n",
      "    maxiter = 1000\n",
      "    \n",
      "    theta1_0 = rand_init(input_layer_size, hidden_layer_size)\n",
      "    theta2_0 = rand_init(hidden_layer_size, num_labels)\n",
      "    thetas0 = pack_thetas(theta1_0, theta2_0)\n",
      "    \n",
      "    thetas = thetas0\n",
      "    alpha = 0.01\n",
      "    diff = 1\n",
      "    prevJ = 1000\n",
      "    tol = 0.000001\n",
      "    i = 0\n",
      "    for _X, _y in minibatch(X, y, batch_size):\n",
      "        thetas = thetas - alpha * function_prime(thetas, input_layer_size, hidden_layer_size, num_labels, _X, _y, reg_lambda)\n",
      "        newJ = float(function(thetas, input_layer_size, hidden_layer_size, num_labels, _X, _y, reg_lambda))\n",
      "        if not np.isnan(newJ) and newJ != float(\"inf\"):\n",
      "            diff = np.abs(newJ - prevJ)\n",
      "            \n",
      "            prevJ = newJ\n",
      "            if diff < tol or i >= maxiter:\n",
      "                break\n",
      "        alpha = alpha * 0.9 if alpha > 0.001 else alpha\n",
      "        i += 1\n",
      "    print tol, i\n",
      "    \n",
      "    return thetas"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit fit(X, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1e-06 338\n",
        "1e-06"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1000\n",
        "1e-06"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1000\n",
        "1e-06"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1000\n",
        "1 loops, best of 3: 104 s per loop\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def minibatch_numba_fit(X, y, batch_size):\n",
      "    m = X.shape[0]\n",
      "    batch_size = int(math.floor(m * batch_size))\n",
      "    max_batchs = int(math.floor(m / batch_size))\n",
      "    \n",
      "    indices = np.random.choice(np.arange(m), m, replace=False)\n",
      "    X, y = X[indices], y[indices]\n",
      "    while True:\n",
      "        for i in range(max_batchs):\n",
      "            indices =  np.arange(i*batch_size, (i + 1) * batch_size)\n",
      "            yield np.hstack((X[indices], y[indices][:, np.newaxis]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "@jit(numba.void(float64[:, :], float64[:]))\n",
      "def fit_numba(X, y):\n",
      "    batch_size = 0.1\n",
      "    reg_lambda = 0\n",
      "    maxiter = 1000\n",
      "    \n",
      "    theta1_0 = rand_init(input_layer_size, hidden_layer_size)\n",
      "    theta2_0 = rand_init(hidden_layer_size, num_labels)\n",
      "    thetas0 = pack_thetas(theta1_0, theta2_0)\n",
      "    \n",
      "    thetas = thetas0\n",
      "    alpha = 0.01\n",
      "    diff = 1\n",
      "    prevJ = 1000\n",
      "    tol = 0.000001\n",
      "    iteration = 0\n",
      "    batches = minibatch_numba_fit(X, y, batch_size)\n",
      "    while diff > tol and iteration < maxiter:\n",
      "        batch = batches.next()\n",
      "        _X = batch[:, :-1]\n",
      "        _y = batch[:, -1].astype(int)\n",
      "        thetas = thetas - alpha * function_prime_numba(thetas, input_layer_size, hidden_layer_size, num_labels, _X, _y, reg_lambda)\n",
      "        newJ = float(function_numba(thetas, input_layer_size, hidden_layer_size, num_labels, _X, _y, reg_lambda))\n",
      "        if not np.isnan(newJ) and newJ < 100000:\n",
      "            diff = np.abs(newJ - prevJ)\n",
      "            prevJ = newJ\n",
      "        alpha = alpha * 0.9 if alpha > 0.001 else alpha\n",
      "        iteration += 1\n",
      "    print tol, iteration\n",
      "    #return thetas"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit fit_numba(X, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1e-06"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1000\n",
        "1e-06"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1000\n",
        "1e-06"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1000\n",
        "1e-06"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1000\n",
        "1 loops, best of 3: 101 s per loop\n"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NN_1HL_MiniBatch(object):\n",
      "    \n",
      "    def __init__(self, reg_lambda=0, epsilon_init=0.12, hidden_layer_size=25, batch_size=0.1, maxiter=1000):\n",
      "        self.reg_lambda = reg_lambda\n",
      "        self.epsilon_init = epsilon_init\n",
      "        self.hidden_layer_size = hidden_layer_size\n",
      "        self.activation_func = self.sigmoid\n",
      "        self.activation_func_prime = self.sigmoid_prime\n",
      "        self.batch_size = batch_size\n",
      "        self.maxiter = maxiter\n",
      "    \n",
      "\n",
      "        \n",
      "    def function_prime(self, thetas, input_layer_size, hidden_layer_size, num_labels, X, y, reg_lambda):\n",
      "        t1, t2 = self.unpack_thetas(thetas, input_layer_size, hidden_layer_size, num_labels)\n",
      "        \n",
      "        m = X.shape[0]\n",
      "        t1f = t1[:, 1:]\n",
      "        t2f = t2[:, 1:]\n",
      "        Y = np.eye(num_labels)[y]\n",
      "        \n",
      "        Delta1, Delta2 = 0, 0\n",
      "        for i, row in enumerate(X):\n",
      "            a1, z2, a2, z3, a3 = self._forward(row, t1, t2)\n",
      "            \n",
      "            # Backprop\n",
      "            d3 = a3 - Y[i, :].T\n",
      "            d2 = np.dot(t2f.T, d3) * self.activation_func_prime(z2)\n",
      "            \n",
      "            Delta2 += np.dot(d3[np.newaxis].T, a2[np.newaxis])\n",
      "            Delta1 += np.dot(d2[np.newaxis].T, a1[np.newaxis])\n",
      "            \n",
      "        Theta1_grad = (1 / m) * Delta1\n",
      "        Theta2_grad = (1 / m) * Delta2\n",
      "        \n",
      "        if reg_lambda != 0:\n",
      "            Theta1_grad[:, 1:] = Theta1_grad[:, 1:] + (reg_lambda / m) * t1f\n",
      "            Theta2_grad[:, 1:] = Theta2_grad[:, 1:] + (reg_lambda / m) * t2f\n",
      "        \n",
      "        return self.pack_thetas(Theta1_grad, Theta2_grad)\n",
      "    \n",
      "    def fit(self, X, y):\n",
      "        input_layer_size = X.shape[1]\n",
      "        num_labels = len(set(y))\n",
      "        \n",
      "        theta1_0 = self.rand_init(input_layer_size, self.hidden_layer_size)\n",
      "        theta2_0 = self.rand_init(self.hidden_layer_size, num_labels)\n",
      "        thetas0 = self.pack_thetas(theta1_0, theta2_0)\n",
      "        \n",
      "        thetas = thetas0\n",
      "        alpha = 0.01\n",
      "        diff = 1\n",
      "        prevJ = 1000\n",
      "        tol = 0.000001\n",
      "        i = 0\n",
      "        for _X, _y in self.minibatch(X, y):\n",
      "            thetas = thetas - alpha * self.function_prime(thetas, input_layer_size, self.hidden_layer_size, num_labels, _X, _y, self.reg_lambda)\n",
      "            newJ = float(self.function(thetas, input_layer_size, self.hidden_layer_size, num_labels, _X, _y, self.reg_lambda))\n",
      "            if not np.isnan(newJ) and newJ != float(\"inf\"):\n",
      "                diff = np.abs(newJ - prevJ)\n",
      "                \n",
      "                prevJ = newJ\n",
      "                if diff < tol or i >= self.maxiter:\n",
      "                    break\n",
      "            alpha = alpha * 0.9 if alpha > 0.001 else alpha\n",
      "            i += 1\n",
      "        print tol, i\n",
      "        \n",
      "        self.t1, self.t2 = self.unpack_thetas(thetas, input_layer_size, self.hidden_layer_size, num_labels)\n",
      "    \n",
      "    def predict(self, X):\n",
      "        return self.predict_proba(X).argmax(0)\n",
      "    \n",
      "    def predict_proba(self, X):\n",
      "        _, _, _, _, h = self._forward(X, self.t1, self.t2)\n",
      "        return h"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}