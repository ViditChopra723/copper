{
 "metadata": {
  "name": "v3_5-dev"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Version 3.5 adds pretraining"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math\n",
      "import numpy as np\n",
      "from scipy import optimize"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import datasets\n",
      "iris = datasets.load_iris()\n",
      "X = iris.data\n",
      "y = iris.target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Logistic Layer"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def softmax(x):\n",
      "    e = np.exp(x - np.max(x))  # prevent overflow\n",
      "    if e.ndim == 1:\n",
      "        return e / np.sum(e, axis=0)\n",
      "    else:  \n",
      "        return e / np.array([np.sum(e, axis=1)]).T  # ndim = 2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Layer(object):\n",
      "    def __init__(self, n_in=None, n_out=None, W=None, random_state=None, activation=None):\n",
      "        if random_state is None:\n",
      "            rnd = np.random.RandomState()\n",
      "        else:\n",
      "            rnd = random_state\n",
      "        \n",
      "        if W is None:\n",
      "            self.W = rnd.uniform(size=(n_in + 1, n_out))\n",
      "        else:\n",
      "            self.W = W\n",
      "        \n",
      "        self.activation = activation\n",
      "        \n",
      "    def output(self, input):\n",
      "        data = np.insert(input, 0, 1, axis=1)\n",
      "        linear_output = np.dot(data, self.W)\n",
      "        return self.activation(linear_output)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class LogisticLayer(Layer):\n",
      "    def __init__(self, n_in=None, n_out=None, W=None, random_state=None):\n",
      "        Layer.__init__(self, n_in, n_out, W, random_state, activation=softmax)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "loglay = LogisticLayer(4, 25)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "loglay.W.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "(5, 25)"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "loglay.output(X).shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "(150, 25)"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Hidden/Sigmoid Layer"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sigmoid(z):\n",
      "    return np.divide(1., (1 + np.exp(-z)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class SigmoidLayer(Layer):\n",
      "    def __init__(self, n_in=None, n_out=None, W=None, random_state=None):\n",
      "        Layer.__init__(self, n_in, n_out, W, random_state, activation=sigmoid)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "siglay = SigmoidLayer(4, 25)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "siglay.W.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "(5, 25)"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "siglay.output(X).shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "(150, 25)"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Network"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "layer_sizes = [25, 30]\n",
      "layer_sizes.insert(0, X.shape[1])\n",
      "layer_sizes.insert(len(layer_sizes), len(np.unique(y)))\n",
      "layer_sizes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "[4, 25, 30, 3]"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def build_network(layers):\n",
      "    return [(layers[i], layers[i + 1]) for i in range(len(layers) - 1)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_weights(layer_sizes, random_state):\n",
      "    weights_info = [(layer_sizes[i] + 1, layer_sizes[i + 1]) for i in range(len(layer_sizes) - 1)]\n",
      "    w_size = 0\n",
      "    for layer_info in weights_info:\n",
      "        w_size += layer_info[0] * layer_info[1]\n",
      "    return random_state.uniform(size=w_size), weights_info"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "weights0, weights_info = create_weights(layer_sizes, np.random.RandomState(1234))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "weights_info,weights0.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "([(5, 25), (26, 30), (31, 3)], (998,))"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_layers(weights, weights_info):\n",
      "    layers = []\n",
      "    # Unpack the weights and assing them\n",
      "    start_pos = 0\n",
      "    for w_info in weights_info:\n",
      "        end_pos = start_pos + w_info[0] * (w_info[1])\n",
      "        weight = weights[start_pos:end_pos].reshape((w_info[0], w_info[1]))\n",
      "        layers.append(SigmoidLayer(W=weight))\n",
      "        start_pos = end_pos\n",
      "    return layers"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "layers = create_layers(weights0, weights_info)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[l.W.shape for l in layers]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "[(5, 25), (26, 30), (31, 3)]"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def forward(layers, X):  # aka predict_proba\n",
      "    output = layers[0].output(X)\n",
      "    for layer in layers[1:]:\n",
      "        output = layer.output(output)\n",
      "    return output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "forward(layers, X).shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "(150, 3)"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "weights0[0] = 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "layers[0].W[:2, :5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "array([[ 1.        ,  0.62210877,  0.43772774,  0.78535858,  0.77997581],\n",
        "       [ 0.65137814,  0.39720258,  0.78873014,  0.31683612,  0.56809865]])"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Pretraining"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "weights0, weights_info = create_weights(layer_sizes, np.random.RandomState(1234))\n",
      "layers = create_layers(weights0, weights_info)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class RBM_Basic(object):\n",
      "    \n",
      "    def __init__(self, input_layer, learning_rate=0.1, random_state=None):\n",
      "        self.input_layer = input_layer\n",
      "        self.learning_rate = learning_rate\n",
      "        \n",
      "        if random_state is None:\n",
      "            rnd = np.random.RandomState()\n",
      "        else:\n",
      "            rnd = random_state\n",
      "        \n",
      "        self.num_visible = input_layer.W.shape[0]\n",
      "        self.num_hidden = input_layer.W.shape[1]\n",
      "        self.W = input_layer.W\n",
      "        \n",
      "    def train(self, data, max_epochs = 1000):\n",
      "        num_examples = data.shape[0]\n",
      "    \n",
      "        # Insert bias units of 1 into the first column.\n",
      "        data = np.insert(data, 0, 1, axis = 1)\n",
      "    \n",
      "        for epoch in range(max_epochs):      \n",
      "            # Clamp to the data and sample from the hidden units. \n",
      "            # (This is the \"positive CD phase\", aka the reality phase.)\n",
      "            pos_hidden_activations = np.dot(data, self.W)      \n",
      "            pos_hidden_probs = sigmoid(pos_hidden_activations)\n",
      "            pos_hidden_states = pos_hidden_probs > np.random.rand(num_examples, self.num_hidden)\n",
      "            # Note that we're using the activation *probabilities* of the hidden states, not the hidden states       \n",
      "            # themselves, when computing associations. We could also use the states; see section 3 of Hinton's \n",
      "            # \"A Practical Guide to Training Restricted Boltzmann Machines\" for more.\n",
      "            pos_associations = np.dot(data.T, pos_hidden_probs)\n",
      "            \n",
      "            # Reconstruct the visible units and sample again from the hidden units.\n",
      "            # (This is the \"negative CD phase\", aka the daydreaming phase.)\n",
      "            neg_visible_activations = np.dot(pos_hidden_states, self.W.T)\n",
      "            neg_visible_probs = sigmoid(neg_visible_activations)\n",
      "            neg_visible_probs[:,0] = 1 # Fix the bias unit.\n",
      "            neg_hidden_activations = np.dot(neg_visible_probs, self.W)\n",
      "            neg_hidden_probs = sigmoid(neg_hidden_activations)\n",
      "            # Note, again, that we're using the activation *probabilities* when computing associations, not the states \n",
      "            # themselves.\n",
      "            neg_associations = np.dot(neg_visible_probs.T, neg_hidden_probs)\n",
      "            \n",
      "            # Update weights.\n",
      "            self.W += self.learning_rate * ((pos_associations - neg_associations) / num_examples)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class RBM(object):\n",
      "    \n",
      "    def __init__(self, input_layer, learning_rate=0.1, random_state=None):\n",
      "        self.input_layer = input_layer\n",
      "        self.learning_rate = learning_rate\n",
      "        \n",
      "        if random_state is None:\n",
      "            self.np_rng = np.random.RandomState()\n",
      "        else:\n",
      "            self.np_rng = random_state\n",
      "        \n",
      "        self.n_visible = input_layer.W.shape[0]\n",
      "        self.n_hidden = input_layer.W.shape[1]\n",
      "        self.W = input_layer.W\n",
      "        \n",
      "        self.hbias = np.zeros(self.n_hidden)  # initialize h bias 0\n",
      "        self.vbias = np.zeros(self.n_visible)  # initialize v bias 0\n",
      "        \n",
      "\n",
      "    def contrastive_divergence(self, input, lr=0.1, k=1):\n",
      "        input = np.insert(input, 0, 1, axis = 1)\n",
      "        ph_mean, ph_sample = self.sample_h_given_v(input)\n",
      "\n",
      "        chain_start = ph_sample\n",
      "\n",
      "        for step in xrange(k):\n",
      "            if step == 0:\n",
      "                nv_means, nv_samples, nh_means, nh_samples = self.gibbs_hvh(chain_start)\n",
      "            else:\n",
      "                nv_means, nv_samples, nh_means, nh_samples = self.gibbs_hvh(nh_samples)\n",
      "\n",
      "\n",
      "        self.W += lr * (np.dot(input.T, ph_sample)\n",
      "                        - np.dot(nv_samples.T, nh_means))\n",
      "        self.vbias += lr * np.mean(input - nv_samples, axis=0)\n",
      "        self.hbias += lr * np.mean(ph_sample - nh_means, axis=0)\n",
      "\n",
      "\n",
      "    def sample_h_given_v(self, v0_sample):\n",
      "        h1_mean = self.propup(v0_sample)\n",
      "        h1_sample = self.np_rng.binomial(size=h1_mean.shape,   # discrete: binomial\n",
      "                                       n=1,\n",
      "                                       p=h1_mean)\n",
      "\n",
      "        return [h1_mean, h1_sample]\n",
      "\n",
      "\n",
      "    def sample_v_given_h(self, h0_sample):\n",
      "        v1_mean = self.propdown(h0_sample)\n",
      "        v1_sample = self.np_rng.binomial(size=v1_mean.shape,   # discrete: binomial\n",
      "                                            n=1,\n",
      "                                            p=v1_mean)\n",
      "        \n",
      "        return [v1_mean, v1_sample]\n",
      "\n",
      "    def propup(self, v):\n",
      "        pre_sigmoid_activation = np.dot(v, self.W)\n",
      "        return sigmoid(pre_sigmoid_activation)\n",
      "\n",
      "    def propdown(self, h):\n",
      "        pre_sigmoid_activation = np.dot(h, self.W.T)\n",
      "        return sigmoid(pre_sigmoid_activation)\n",
      "\n",
      "\n",
      "    def gibbs_hvh(self, h0_sample):\n",
      "        v1_mean, v1_sample = self.sample_v_given_h(h0_sample)\n",
      "        h1_mean, h1_sample = self.sample_h_given_v(v1_sample)\n",
      "\n",
      "        return [v1_mean, v1_sample, h1_mean, h1_sample]\n",
      "    \n",
      "\n",
      "    def get_reconstruction_cross_entropy(self):\n",
      "        pre_sigmoid_activation_h = np.dot(self.input, self.W) + self.hbias\n",
      "        sigmoid_activation_h = sigmoid(pre_sigmoid_activation_h)\n",
      "        \n",
      "        pre_sigmoid_activation_v = np.dot(sigmoid_activation_h, self.W.T) + self.vbias\n",
      "        sigmoid_activation_v = sigmoid(pre_sigmoid_activation_v)\n",
      "\n",
      "        cross_entropy =  - np.mean(\n",
      "            np.sum(self.input * np.log(sigmoid_activation_v) +\n",
      "            (1 - self.input) * np.log(1 - sigmoid_activation_v),\n",
      "                      axis=1))\n",
      "        \n",
      "        return cross_entropy\n",
      "\n",
      "    def reconstruct(self, v):\n",
      "        h = sigmoid(np.dot(v, self.W) + self.hbias)\n",
      "        reconstructed_v = sigmoid(np.dot(h, self.W.T) + self.vbias)\n",
      "        return reconstructed_v"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "layers[0].W[:2, :5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 66,
       "text": [
        "array([[  1.20767739,   1.52224529,   1.06897026,   2.55587122,\n",
        "          1.34966547],\n",
        "       [ 74.31753608,  73.9473391 ,  74.06997267,  71.72734876,\n",
        "         73.78778831]])"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rbm = RBM(layers[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rbm.contrastive_divergence(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(150, 5) (5, 25)\n",
        "(150, 5) (5, 25)\n"
       ]
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rbm.W"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 65,
       "text": [
        "array([[  1.20767739,   1.52224529,   1.06897026,   2.55587122,\n",
        "          1.34966547,   1.2736799 ,   1.84851432,   1.04444666,\n",
        "          1.81758844,   1.13815793,   2.44358914,   1.6873609 ,\n",
        "          1.17963602,   1.49156738,   1.09582375,   1.86359448,\n",
        "          2.20564538,   2.22702972,   1.14573217,   1.55594823,\n",
        "          1.17283585,   1.37516482,   0.91396946,   2.49150109,\n",
        "          1.84551649],\n",
        "       [ 74.31753608,  73.9473391 ,  74.06997267,  71.72734876,\n",
        "         73.78778831,  74.52021469,  74.65822349,  73.69472212,\n",
        "         73.65321591,  73.61648626,  75.44035318,  73.37515788,\n",
        "         74.07104071,  73.87100611,  74.28488896,  63.16220752,\n",
        "         74.5368493 ,  74.91061655,  73.30778649,  73.03793181,\n",
        "         73.99126003,  72.6830927 ,  73.6600213 ,  71.84234553,\n",
        "         73.64534322],\n",
        "       [ 31.93805226,  32.31733023,  32.00718717,  31.2372767 ,\n",
        "         31.99713137,  32.72321019,  33.1725742 ,  32.04465595,\n",
        "         32.62825085,  31.86418943,  33.18102283,  32.29128248,\n",
        "         31.78426688,  31.78454054,  31.91789045,  27.07627198,\n",
        "         32.96421063,  34.00526602,  31.09684825,  31.04268793,\n",
        "         32.35647292,  31.69707227,  31.91022075,  31.44980391,\n",
        "         31.71159495],\n",
        "       [ 43.29612314,  42.69689006,  42.54709419,  41.59672115,\n",
        "         42.25033136,  42.81798047,  43.56419906,  42.54077256,\n",
        "         42.86518576,  42.34822286,  43.61560559,  43.27242918,\n",
        "         42.70718007,  42.79259113,  42.54388288,  38.37497107,\n",
        "         43.65097183,  44.12148555,  42.65433431,  42.45366621,\n",
        "         42.69050943,  42.58664683,  42.98779028,  42.03979273,\n",
        "         42.92179813],\n",
        "       [  4.76327457,   4.58825188,   4.40810971,   5.22827346,\n",
        "          4.51552619,   4.1282442 ,   4.58169706,   3.81646797,\n",
        "          3.95351479,   4.19303514,   5.39147929,   4.51998446,\n",
        "          3.93398473,   4.67926793,   4.58464216,   5.11501405,\n",
        "          5.03057101,   5.37585001,   4.3447016 ,   4.4998352 ,\n",
        "          4.51460833,   4.79985647,   4.68775202,   5.71183187,\n",
        "          4.26350163]])"
       ]
      }
     ],
     "prompt_number": 65
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Cost function and gradient"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "weights0.shape, weights0[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 82,
       "text": [
        "((998,),\n",
        " array([ 0.19151945,  0.62210877,  0.43772774,  0.78535858,  0.77997581,\n",
        "        0.27259261,  0.27646426,  0.80187218,  0.95813935,  0.87593263]))"
       ]
      }
     ],
     "prompt_number": 82
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def unpack_weigths(weights, weights_meta):\n",
      "    start_pos = 0\n",
      "    for layer in weights_meta:\n",
      "        end_pos = start_pos + layer[0] * (layer[1])\n",
      "        yield weights[start_pos:end_pos].reshape((layer[0], layer[1]))\n",
      "        start_pos = end_pos"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 84
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def cost(weights, X, y, weights_meta, num_labels):\n",
      "    # Forward\n",
      "    act_prev = np.insert(X, 0, 1, axis=1)\n",
      "    for weight in unpack_weigths(weights, weights_meta):\n",
      "        z = np.dot(act_prev, weight)\n",
      "        activation = sigmoid(z)\n",
      "        act_prev = np.insert(activation, 0, 1, axis=1)\n",
      "    \n",
      "    Y = np.eye(num_labels)[y]\n",
      "    h = activation\n",
      "    costPositive = -Y * np.log(h)\n",
      "    costNegative = (1 - Y) * np.log(1 - h)\n",
      "    J = np.sum(costPositive - costNegative) / X.shape[0]\n",
      "    \n",
      "    return J"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 85
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cost(weights0, X, y, weights_info, len(np.unique(y)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 86,
       "text": [
        "31.397997599425246"
       ]
      }
     ],
     "prompt_number": 86
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def unpack_weigths_inv(weights, weights_meta):\n",
      "    end_pos = len(weights)\n",
      "    for layer in reversed(weights_meta):\n",
      "        start_pos = end_pos - layer[0] * (layer[1])\n",
      "        yield weights[start_pos:end_pos].reshape((layer[0], layer[1]))\n",
      "        end_pos = start_pos"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 87
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def cost_prime(weights, X, y, weights_meta, num_labels):\n",
      "    Y = np.eye(num_labels)[y]\n",
      "    Deltas = [np.zeros(shape) for shape in weights_meta]\n",
      "    \n",
      "    data = np.insert(X, 0, 1, axis=1)\n",
      "    for i, row in enumerate(data):\n",
      "        # Forward\n",
      "        #row = np.array([row])\n",
      "        act_prev = row\n",
      "        activations = (act_prev, )\n",
      "        for weight in unpack_weigths(weights, weights_meta):\n",
      "            z = np.dot(act_prev, weight)\n",
      "            activation = sigmoid(z)\n",
      "            act_prev = np.append(1, activation)\n",
      "            activations = activations + (act_prev, )\n",
      "        \n",
      "        # Backprop\n",
      "        prev_delta = activations[-1][1:] - Y[i, :].T  # last delta\n",
      "        deltas = (prev_delta, )  # deltas[0] == delta2\n",
      "        for act, weight in zip(reversed(activations[1:-1]), unpack_weigths_inv(weights, weights_meta)):\n",
      "            delta = np.dot(weight, prev_delta)[1:] * (act[1:] * (1 - act[1:])).T\n",
      "            deltas = (delta, ) + deltas\n",
      "            prev_delta = delta\n",
      "\n",
      "        # Accumulate errors\n",
      "        for delta, act, i in zip(deltas, activations[:-1], range(len(Deltas))):\n",
      "            Deltas[i] = Deltas[i] + np.dot(delta[np.newaxis].T, act[np.newaxis]).T\n",
      "    for i in range(len(Deltas)):\n",
      "        Deltas[i] = Deltas[i] / X.shape[0]\n",
      "    return np.concatenate(tuple([D.reshape(-1) for D in Deltas]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 88
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cost_prime(weights0, X, y, weights_info, len(np.unique(y)))[:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 89,
       "text": [
        "array([  1.57880739e-07,   1.09050540e-07,   1.59794342e-08,\n",
        "         2.27139143e-06,   4.51481472e-08,   5.70543666e-09,\n",
        "         7.71786320e-08,   1.75347728e-09,   8.93488019e-08,\n",
        "         5.89878368e-09,   1.71267237e-07,   1.59582181e-07,\n",
        "         6.52428875e-09,   1.73089602e-07,   1.86700579e-08,\n",
        "         5.82387482e-06,   4.18367021e-07,   3.50756385e-07,\n",
        "         4.60059687e-08,   1.54522762e-07])"
       ]
      }
     ],
     "prompt_number": 89
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Optimization"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def minibatches(X, y=None, batch_size=50):\n",
      "    m = X.shape[0]\n",
      "    batch_size = batch_size if batch_size >= 1 else int(math.floor(m * batch_size))\n",
      "    max_batchs = int(math.floor(m / batch_size))\n",
      "    \n",
      "    while True:\n",
      "        indices = np.random.choice(np.arange(m), m, replace=False)\n",
      "        for i in range(max_batchs):\n",
      "            indices = np.arange(i * batch_size, (i + 1) * batch_size)\n",
      "            if y is None:\n",
      "                yield X[indices]\n",
      "            else:\n",
      "                yield X[indices], y[indices]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 90
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mb = minibatches(X, batch_size=5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 91
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mb.next()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 92,
       "text": [
        "array([[ 5.1,  3.5,  1.4,  0.2],\n",
        "       [ 4.9,  3. ,  1.4,  0.2],\n",
        "       [ 4.7,  3.2,  1.3,  0.2],\n",
        "       [ 4.6,  3.1,  1.5,  0.2],\n",
        "       [ 5. ,  3.6,  1.4,  0.2]])"
       ]
      }
     ],
     "prompt_number": 92
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class MinibatchOpti(object):\n",
      "    \n",
      "    @staticmethod\n",
      "    def minibatches(X, y=None, batch_size=50):\n",
      "        m = X.shape[0]\n",
      "        batch_size = batch_size if batch_size >= 1 else int(math.floor(m * batch_size))\n",
      "        max_batchs = int(math.floor(m / batch_size))\n",
      "        \n",
      "        while True:\n",
      "            indices = np.random.choice(np.arange(m), m, replace=False)\n",
      "            for i in range(max_batchs):\n",
      "                indices = np.arange(i * batch_size, (i + 1) * batch_size)\n",
      "                if y is None:\n",
      "                    yield X[indices]\n",
      "                else:\n",
      "                    yield X[indices], y[indices]\n",
      "    @staticmethod\n",
      "    def GD(fun, weights, jac, X, y, options, args=()):\n",
      "        weights2 = weights - options['learning_rate'] * jac(weights, X, y, *args)\n",
      "        options['learning_rate'] = options['learning_rate'] * options['learning_rate_decay']\n",
      "        return weights2\n",
      "    \n",
      "    @staticmethod\n",
      "    def minimize(fun, weights, jac, method=None, batch_size=50, tol=1e-6, maxiter=100, args=None, disp=False, options=None, callback=None):\n",
      "        if method == 'GD':\n",
      "            update = MinibatchOpti.GD\n",
      "        else:\n",
      "            raise 'Optimization method not found'\n",
      "\n",
      "        i = 1\n",
      "        prevJ = 1000\n",
      "        #weights = weights0\n",
      "        for _X, _y in MinibatchOpti.minibatches(X, y, batch_size):\n",
      "            weights[:] = update(fun, weights, jac, _X, _y, options, args=args)\n",
      "            #print weights[:10]\n",
      "            newJ = float(fun(weights, X, y, *args))\n",
      "            diff = newJ - prevJ\n",
      "            if np.abs(diff) < tol or i >= maxiter:\n",
      "                break\n",
      "            if disp:\n",
      "                print i, newJ    \n",
      "            if callback is not None:\n",
      "                callback(i, weights)\n",
      "            prevJ = newJ\n",
      "            i = i + 1\n",
      "        #return weights0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 93
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "weights0, weights_info = create_weights(layer_sizes, np.random.RandomState(1234))\n",
      "layers = create_layers(weights0, weights_info)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 94
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "weights0[:10] # Original"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 95,
       "text": [
        "array([ 0.19151945,  0.62210877,  0.43772774,  0.78535858,  0.77997581,\n",
        "        0.27259261,  0.27646426,  0.80187218,  0.95813935,  0.87593263])"
       ]
      }
     ],
     "prompt_number": 95
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "layers[0].W[0][:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 96,
       "text": [
        "array([ 0.19151945,  0.62210877,  0.43772774,  0.78535858,  0.77997581,\n",
        "        0.27259261,  0.27646426,  0.80187218,  0.95813935,  0.87593263])"
       ]
      }
     ],
     "prompt_number": 96
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "options = {}\n",
      "options['learning_rate'] = 0.3\n",
      "options['learning_rate_decay'] = 0.8\n",
      "MinibatchOpti.minimize(cost, weights0, cost_prime, method='GD', batch_size=50, disp=True, maxiter=15, args=(weights_info, len(np.unique(y))), options=options)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 6.40479424841\n",
        "2 4.20425666078\n",
        "3 3.41592079539\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3.16363955209\n",
        "5 2.51260714413\n",
        "6 2.23444858473\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2.12636299861\n",
        "8 2.0276163102\n",
        "9 1.97690645084\n",
        "10 1.95427499715\n",
        "11"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.93610871697\n",
        "12 1.92530086175\n",
        "13 1.92060143526\n",
        "14 1.91649440823\n"
       ]
      }
     ],
     "prompt_number": 99
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "layers[0].W[0][:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 100,
       "text": [
        "array([ 0.19151934,  0.62210867,  0.43772772,  0.78535667,  0.77997577,\n",
        "        0.2725926 ,  0.2764642 ,  0.80187218,  0.95813929,  0.87593263])"
       ]
      }
     ],
     "prompt_number": 100
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def predict(layers, X):\n",
      "    return forward(layers, X).argmax(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 101
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predict(layers, X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 102,
       "text": [
        "array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
        "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
        "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
        "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
        "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
        "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
        "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
       ]
      }
     ],
     "prompt_number": 102
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}