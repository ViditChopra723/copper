{
 "metadata": {
  "name": "v3_5-dev"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Version 3.5 adds pretraining"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import datasets\n",
      "iris = datasets.load_iris()\n",
      "X = iris.data\n",
      "y = iris.target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math\n",
      "import numpy as np\n",
      "from scipy import optimize"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.io import loadmat"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = loadmat('ex3weights.mat')\n",
      "t1 = data['Theta1']\n",
      "t2 = data['Theta2']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = loadmat('ex3data1.mat')\n",
      "X = data['X']\n",
      "y = data['y'].reshape(-1) - 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 69
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Logistic Layer"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def softmax(x):\n",
      "    e = np.exp(x - np.max(x))  # prevent overflow\n",
      "    if e.ndim == 1:\n",
      "        return e / np.sum(e, axis=0)\n",
      "    else:  \n",
      "        return e / np.array([np.sum(e, axis=1)]).T  # ndim = 2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Layer(object):\n",
      "    def __init__(self, n_in=None, n_out=None, W=None, random_state=None, activation=None):\n",
      "        if random_state is None:\n",
      "            rnd = np.random.RandomState()\n",
      "        else:\n",
      "            rnd = random_state\n",
      "        \n",
      "        if W is None:\n",
      "            self.W = rnd.uniform(size=(n_in + 1, n_out))\n",
      "        else:\n",
      "            self.W = W\n",
      "        \n",
      "        self.activation = activation\n",
      "        \n",
      "    def output(self, input):\n",
      "        data = np.insert(input, 0, 1, axis=1)\n",
      "        linear_output = np.dot(data, self.W)\n",
      "        return self.activation(linear_output)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class LogisticLayer(Layer):\n",
      "    def __init__(self, n_in=None, n_out=None, W=None, random_state=None):\n",
      "        Layer.__init__(self, n_in, n_out, W, random_state, activation=softmax)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "loglay = LogisticLayer(400, 25)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "loglay.W.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "(401, 25)"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "(5000, 400)"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "loglay.output(X).shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "(5000, 25)"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Hidden/Sigmoid Layer"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sigmoid(z):\n",
      "    return np.divide(1., (1 + np.exp(-z)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class SigmoidLayer(Layer):\n",
      "    def __init__(self, n_in=None, n_out=None, W=None, random_state=None):\n",
      "        Layer.__init__(self, n_in, n_out, W, random_state, activation=sigmoid)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "siglay = SigmoidLayer(400, 25)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "siglay.W.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "(401, 25)"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "siglay.output(X).shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 26,
       "text": [
        "(5000, 25)"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Network"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "layer_sizes = [25]\n",
      "layer_sizes.insert(0, X.shape[1])\n",
      "layer_sizes.insert(len(layer_sizes), len(np.unique(y)))\n",
      "layer_sizes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 27,
       "text": [
        "[400, 25, 10]"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def build_network(layers):\n",
      "    return [(layers[i], layers[i + 1]) for i in range(len(layers) - 1)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_weights(layer_sizes, random_state):\n",
      "    weights_info = [(layer_sizes[i] + 1, layer_sizes[i + 1]) for i in range(len(layer_sizes) - 1)]\n",
      "    w_size = 0\n",
      "    for layer_info in weights_info:\n",
      "        w_size += layer_info[0] * layer_info[1]\n",
      "    return random_state.uniform(size=w_size), weights_info"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "weights0, weights_info = create_weights(layer_sizes, np.random.RandomState(1234))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "weights_info,weights0.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 33,
       "text": [
        "([(401, 25), (26, 10)], (10285,))"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_layers(weights, weights_info):\n",
      "    layers = []\n",
      "    # Unpack the weights and assing them\n",
      "    start_pos = 0\n",
      "    for w_info in weights_info:\n",
      "        end_pos = start_pos + w_info[0] * (w_info[1])\n",
      "        weight = weights[start_pos:end_pos].reshape((w_info[0], w_info[1]))\n",
      "        layers.append(SigmoidLayer(W=weight))\n",
      "        start_pos = end_pos\n",
      "    return layers"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "layers = create_layers(weights0, weights_info)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[l.W.shape for l in layers]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 36,
       "text": [
        "[(401, 25), (26, 10)]"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def forward(layers, X):  # aka predict_proba\n",
      "    output = layers[0].output(X)\n",
      "    for layer in layers[1:]:\n",
      "        output = layer.output(output)\n",
      "    return output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "forward(layers, X).shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 38,
       "text": [
        "(5000, 10)"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "weights0[0] = 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "layers[0].W[:2, :5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 40,
       "text": [
        "array([[ 1.        ,  0.62210877,  0.43772774,  0.78535858,  0.77997581],\n",
        "       [ 0.65137814,  0.39720258,  0.78873014,  0.31683612,  0.56809865]])"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Cost function and gradient"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "weights0, weights_info = create_weights(layer_sizes, np.random.RandomState(1234))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "weights0 = np.concatenate(tuple([D.reshape(-1) for D in [t1.T, t2.T]]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "weights0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 80,
       "text": [
        "array([-0.02256239, -0.09838113,  0.11615605, ..., -2.11014003,\n",
        "       -2.1319153 , -1.32752042])"
       ]
      }
     ],
     "prompt_number": 80
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "layers = create_layers(weights0, weights_info)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t1.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 59,
       "text": [
        "(25, 401)"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(layers[0].W == t1.T).all()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 62,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def unpack_weigths(weights, weights_meta):\n",
      "    start_pos = 0\n",
      "    for layer in weights_meta:\n",
      "        end_pos = start_pos + layer[0] * (layer[1])\n",
      "        yield weights[start_pos:end_pos].reshape((layer[0], layer[1]))\n",
      "        start_pos = end_pos"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def cost(weights, X, y, weights_meta, num_labels):\n",
      "    # Forward\n",
      "    act_prev = np.insert(X, 0, 1, axis=1)\n",
      "    for weight in unpack_weigths(weights, weights_meta):\n",
      "        z = np.dot(act_prev, weight)\n",
      "        activation = sigmoid(z)\n",
      "        act_prev = np.insert(activation, 0, 1, axis=1)\n",
      "    \n",
      "    Y = np.eye(num_labels)[y]\n",
      "    h = activation\n",
      "    costPositive = -Y * np.log(h)\n",
      "    costNegative = (1 - Y) * np.log(1 - h)\n",
      "    J = np.sum(costPositive - costNegative) / X.shape[0]\n",
      "    \n",
      "    return J"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cost(weights0, X, y, weights_info, len(np.unique(y)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 72,
       "text": [
        "0.28762916516132031"
       ]
      }
     ],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def unpack_weigths_inv(weights, weights_meta):\n",
      "    end_pos = len(weights)\n",
      "    for layer in reversed(weights_meta):\n",
      "        start_pos = end_pos - layer[0] * (layer[1])\n",
      "        yield weights[start_pos:end_pos].reshape((layer[0], layer[1]))\n",
      "        end_pos = start_pos"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 73
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def cost_prime(weights, X, y, weights_meta, num_labels):\n",
      "    Y = np.eye(num_labels)[y]\n",
      "    Deltas = [np.zeros(shape) for shape in weights_meta]\n",
      "    \n",
      "    data = np.insert(X, 0, 1, axis=1)\n",
      "    for i, row in enumerate(data):\n",
      "        # Forward\n",
      "        #row = np.array([row])\n",
      "        act_prev = row\n",
      "        activations = (act_prev, )\n",
      "        for weight in unpack_weigths(weights, weights_meta):\n",
      "            z = np.dot(act_prev, weight)\n",
      "            activation = sigmoid(z)\n",
      "            act_prev = np.append(1, activation)\n",
      "            activations = activations + (act_prev, )\n",
      "        #print ([d.shape for d in activations])\n",
      "        # Backprop\n",
      "        prev_delta = activations[-1][1:] - Y[i, :].T  # last delta\n",
      "        deltas = (prev_delta, )  # deltas[0] == delta2\n",
      "        for act, weight in zip(reversed(activations[1:-1]), unpack_weigths_inv(weights, weights_meta)):\n",
      "            delta = np.dot(weight, prev_delta)[1:] * (act[1:] * (1 - act[1:])).T\n",
      "            deltas = (delta, ) + deltas\n",
      "            prev_delta = delta\n",
      "        #print ([d.shape for d in deltas])\n",
      "        # Accumulate errors\n",
      "        for delta, act, i in zip(deltas, activations[:-1], range(len(Deltas))):\n",
      "            Deltas[i] = Deltas[i] + np.dot(delta[np.newaxis].T, act[np.newaxis]).T\n",
      "    for i in range(len(Deltas)):\n",
      "        Deltas[i] = Deltas[i] / X.shape[0]\n",
      "    return np.concatenate(tuple([D.reshape(-1) for D in Deltas]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 75
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cost_prime(weights0, X, y, weights_info, len(np.unique(y)))[-20:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 81,
       "text": [
        "array([ 0.00050715,  0.0014473 , -0.00055127,  0.00120362,  0.00043575,\n",
        "        0.00100404, -0.00097081, -0.00037581,  0.00113453, -0.00075774,\n",
        "        0.00087367, -0.00090376,  0.00032189,  0.00087981, -0.0015519 ,\n",
        "        0.00050005, -0.00098491,  0.00127957,  0.00135707,  0.00077333])"
       ]
      }
     ],
     "prompt_number": 81
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Optimization"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def minibatches(X, y=None, batch_size=50):\n",
      "    m = X.shape[0]\n",
      "    batch_size = batch_size if batch_size >= 1 else int(math.floor(m * batch_size))\n",
      "    max_batchs = int(math.floor(m / batch_size))\n",
      "    \n",
      "    while True:\n",
      "        indices = np.random.choice(np.arange(m), m, replace=False)\n",
      "        for i in range(max_batchs):\n",
      "            indices = np.arange(i * batch_size, (i + 1) * batch_size)\n",
      "            if y is None:\n",
      "                yield X[indices]\n",
      "            else:\n",
      "                yield X[indices], y[indices]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 90
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mb = minibatches(X, batch_size=5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 91
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mb.next()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 92,
       "text": [
        "array([[ 5.1,  3.5,  1.4,  0.2],\n",
        "       [ 4.9,  3. ,  1.4,  0.2],\n",
        "       [ 4.7,  3.2,  1.3,  0.2],\n",
        "       [ 4.6,  3.1,  1.5,  0.2],\n",
        "       [ 5. ,  3.6,  1.4,  0.2]])"
       ]
      }
     ],
     "prompt_number": 92
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class MinibatchOpti(object):\n",
      "    \n",
      "    @staticmethod\n",
      "    def minibatches(X, y=None, batch_size=50):\n",
      "        m = X.shape[0]\n",
      "        batch_size = batch_size if batch_size >= 1 else int(math.floor(m * batch_size))\n",
      "        max_batchs = int(math.floor(m / batch_size))\n",
      "        \n",
      "        while True:\n",
      "            indices = np.random.choice(np.arange(m), m, replace=False)\n",
      "            for i in range(max_batchs):\n",
      "                indices = np.arange(i * batch_size, (i + 1) * batch_size)\n",
      "                if y is None:\n",
      "                    yield X[indices]\n",
      "                else:\n",
      "                    yield X[indices], y[indices]\n",
      "    @staticmethod\n",
      "    def GD(fun, weights, jac, X, y, options, args=()):\n",
      "        weights2 = weights - options['learning_rate'] * jac(weights, X, y, *args)\n",
      "        options['learning_rate'] = options['learning_rate'] * options['learning_rate_decay']\n",
      "        return weights2\n",
      "    \n",
      "    @staticmethod\n",
      "    def minimize(fun, weights, jac, method=None, batch_size=50, tol=1e-6, maxiter=100, args=None, disp=False, options=None, callback=None):\n",
      "        if method == 'GD':\n",
      "            update = MinibatchOpti.GD\n",
      "        else:\n",
      "            raise 'Optimization method not found'\n",
      "\n",
      "        i = 1\n",
      "        prevJ = 1000\n",
      "        #weights = weights0\n",
      "        for _X, _y in MinibatchOpti.minibatches(X, y, batch_size):\n",
      "            weights[:] = update(fun, weights, jac, _X, _y, options, args=args)\n",
      "            #print weights[:10]\n",
      "            newJ = float(fun(weights, X, y, *args))\n",
      "            diff = newJ - prevJ\n",
      "            if np.abs(diff) < tol or i >= maxiter:\n",
      "                break\n",
      "            if disp:\n",
      "                print i, newJ    \n",
      "            if callback is not None:\n",
      "                callback(i, weights)\n",
      "            prevJ = newJ\n",
      "            i = i + 1\n",
      "        #return weights0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 93
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "weights0, weights_info = create_weights(layer_sizes, np.random.RandomState(1234))\n",
      "layers = create_layers(weights0, weights_info)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 94
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "weights0[:10] # Original"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 95,
       "text": [
        "array([ 0.19151945,  0.62210877,  0.43772774,  0.78535858,  0.77997581,\n",
        "        0.27259261,  0.27646426,  0.80187218,  0.95813935,  0.87593263])"
       ]
      }
     ],
     "prompt_number": 95
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "layers[0].W[0][:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 96,
       "text": [
        "array([ 0.19151945,  0.62210877,  0.43772774,  0.78535858,  0.77997581,\n",
        "        0.27259261,  0.27646426,  0.80187218,  0.95813935,  0.87593263])"
       ]
      }
     ],
     "prompt_number": 96
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "options = {}\n",
      "options['learning_rate'] = 0.3\n",
      "options['learning_rate_decay'] = 0.8\n",
      "MinibatchOpti.minimize(cost, weights0, cost_prime, method='GD', batch_size=50, disp=True, maxiter=15, args=(weights_info, len(np.unique(y))), options=options)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 6.40479424841\n",
        "2 4.20425666078\n",
        "3 3.41592079539\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3.16363955209\n",
        "5 2.51260714413\n",
        "6 2.23444858473\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2.12636299861\n",
        "8 2.0276163102\n",
        "9 1.97690645084\n",
        "10 1.95427499715\n",
        "11"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.93610871697\n",
        "12 1.92530086175\n",
        "13 1.92060143526\n",
        "14 1.91649440823\n"
       ]
      }
     ],
     "prompt_number": 99
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "layers[0].W[0][:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 100,
       "text": [
        "array([ 0.19151934,  0.62210867,  0.43772772,  0.78535667,  0.77997577,\n",
        "        0.2725926 ,  0.2764642 ,  0.80187218,  0.95813929,  0.87593263])"
       ]
      }
     ],
     "prompt_number": 100
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def predict(layers, X):\n",
      "    return forward(layers, X).argmax(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 101
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predict(layers, X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 102,
       "text": [
        "array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
        "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
        "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
        "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
        "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
        "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
        "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
       ]
      }
     ],
     "prompt_number": 102
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}