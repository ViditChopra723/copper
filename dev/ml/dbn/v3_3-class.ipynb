{
 "metadata": {
  "name": "v3_3-class"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math\n",
      "import numpy as np\n",
      "from scipy import optimize"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Logistic Layer"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Layer(object):\n",
      "    def __init__(self, n_in=None, n_out=None, W=None, random_state=None, activation=None):\n",
      "        if random_state is None:\n",
      "            rnd = np.random.RandomState()\n",
      "        else:\n",
      "            rnd = random_state\n",
      "        \n",
      "        if W is None:\n",
      "            self.W = rnd.uniform(size=(n_in + 1, n_out))\n",
      "        else:\n",
      "            self.W = W\n",
      "        \n",
      "        self.activation = activation\n",
      "        \n",
      "    def output(self, input):\n",
      "        data = np.insert(input, 0, 1, axis=1)\n",
      "        linear_output = np.dot(data, self.W)\n",
      "        return self.activation(linear_output)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def softmax(x):\n",
      "    e = np.exp(x - np.max(x))  # prevent overflow\n",
      "    if e.ndim == 1:\n",
      "        return e / np.sum(e, axis=0)\n",
      "    else:  \n",
      "        return e / np.array([np.sum(e, axis=1)]).T  # ndim = 2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class LogisticLayer(Layer):\n",
      "    def __init__(self, n_in=None, n_out=None, W=None, random_state=None):\n",
      "        Layer.__init__(self, n_in, n_out, W, random_state, activation=softmax)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Sigmoid Layer"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sigmoid(z):\n",
      "    return np.divide(1., (1 + np.exp(-z)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class SigmoidLayer(Layer):\n",
      "    def __init__(self, n_in=None, n_out=None, W=None, random_state=None):\n",
      "        Layer.__init__(self, n_in, n_out, W, random_state, activation=sigmoid)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Cost function and gradient"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def unpack_weigths(weights, weights_meta):\n",
      "    start_pos = 0\n",
      "    for layer in weights_meta:\n",
      "        end_pos = start_pos + layer[0] * (layer[1])\n",
      "        yield weights[start_pos:end_pos].reshape((layer[0], layer[1]))\n",
      "        start_pos = end_pos"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def cost(weights, X, y, weights_meta, num_labels):\n",
      "    # Forward\n",
      "    act_prev = np.insert(X, 0, 1, axis=1)\n",
      "    for weight in unpack_weigths(weights, weights_meta):\n",
      "        z = np.dot(act_prev, weight)\n",
      "        activation = sigmoid(z)\n",
      "        act_prev = np.insert(activation, 0, 1, axis=1)\n",
      "    \n",
      "    Y = np.eye(num_labels)[y]\n",
      "    h = activation\n",
      "    costPositive = -Y * np.log(h)\n",
      "    costNegative = (1 - Y) * np.log(1 - h)\n",
      "    J = np.sum(costPositive - costNegative) / X.shape[0]\n",
      "    \n",
      "    return J"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def unpack_weigths_inv(weights, weights_meta):\n",
      "    end_pos = len(weights)\n",
      "    for layer in reversed(weights_meta):\n",
      "        start_pos = end_pos - layer[0] * (layer[1])\n",
      "        yield weights[start_pos:end_pos].reshape((layer[0], layer[1]))\n",
      "        end_pos = start_pos"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def cost_prime(weights, X, y, weights_meta, num_labels):\n",
      "    Y = np.eye(num_labels)[y]\n",
      "    Deltas = [np.zeros(shape) for shape in weights_meta]\n",
      "    \n",
      "    data = np.insert(X, 0, 1, axis=1)\n",
      "    for i, row in enumerate(data):\n",
      "        # Forward\n",
      "        #row = np.array([row])\n",
      "        act_prev = row\n",
      "        activations = (act_prev, )\n",
      "        for weight in unpack_weigths(weights, weights_meta):\n",
      "            z = np.dot(act_prev, weight)\n",
      "            activation = sigmoid(z)\n",
      "            act_prev = np.append(1, activation)\n",
      "            activations = activations + (act_prev, )\n",
      "        \n",
      "        # Backprop\n",
      "        prev_delta = activations[-1][1:] - Y[i, :].T  # last delta\n",
      "        deltas = (prev_delta, )  # deltas[0] == delta2\n",
      "        for act, weight in zip(reversed(activations[1:-1]), unpack_weigths_inv(weights, weights_meta)):\n",
      "            delta = np.dot(weight, prev_delta)[1:] * (act[1:] * (1 - act[1:])).T\n",
      "            deltas = (delta, ) + deltas\n",
      "            prev_delta = delta\n",
      "\n",
      "        # Accumulate errors\n",
      "        for delta, act, i in zip(deltas, activations[:-1], range(len(Deltas))):\n",
      "            Deltas[i] = Deltas[i] + np.dot(delta[np.newaxis].T, act[np.newaxis]).T\n",
      "    for i in range(len(Deltas)):\n",
      "        Deltas[i] = Deltas[i] / X.shape[0]\n",
      "    return np.concatenate(tuple([D.reshape(-1) for D in Deltas]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Optimization"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class MinibatchOpti(object):\n",
      "    \n",
      "    @staticmethod\n",
      "    def minibatches(X, y=None, batch_size=50, random_state=None):\n",
      "        if random_state is None:\n",
      "            rnd = np.random.RandomState()\n",
      "        elif isinstance(random_state, int):\n",
      "            rnd = np.random.RandomState(random_state)\n",
      "        else:\n",
      "           rnd = random_state\n",
      "\n",
      "        m = X.shape[0]\n",
      "        batch_size = batch_size if batch_size >= 1 else int(math.floor(m * batch_size))\n",
      "        max_batchs = int(math.floor(m / batch_size))\n",
      "        random_indices = np.random.choice(np.arange(m), m, replace=False)\n",
      "        while True:\n",
      "            for i in range(max_batchs):\n",
      "                batch_indices = np.arange(i * batch_size, (i + 1) * batch_size)\n",
      "                indices = random_indices[batch_indices]\n",
      "                if y is None:\n",
      "                    yield X[indices]\n",
      "                else:\n",
      "                    yield X[indices], y[indices]\n",
      "    @staticmethod\n",
      "    def GD(fun, weights, jac, X, y, options, args=()):\n",
      "        weights -= options['learning_rate'] * jac(weights, X, y, *args)\n",
      "        options['learning_rate'] = options['learning_rate'] * options['learning_rate_decay']\n",
      "    \n",
      "    @staticmethod\n",
      "    def GD_momentum(fun, weights, jac, X, y, options, args=()):\n",
      "        bigjump = options['momentum'] * options['step']\n",
      "        weights -= bigjump\n",
      "        correction = options['learning_rate'] * jac(weights, X, y, *args)\n",
      "        weights -= correction\n",
      "        options['step'] = bigjump + correction\n",
      "        options['learning_rate'] = options['learning_rate'] * options['learning_rate_decay']\n",
      "        options['momentum'] = options['momemtum_decay'] * options['momentum']\n",
      "        \n",
      "    @staticmethod\n",
      "    def RMSPROP(fun, weights, jac, X, y, options, args=()):\n",
      "        gradient = jac(weights, X, y, *args)\n",
      "        options['moving_mean_squared'] = options['decay'] * options['moving_mean_squared'] \\\n",
      "                                         + (1 - options['decay']) * gradient ** 2\n",
      "        weights -= gradient / np.sqrt(options['moving_mean_squared'] + 1e-8)\n",
      "        \n",
      "    @staticmethod\n",
      "    def CG(fun, weights, jac, X, y, options, args=()):\n",
      "        ans = optimize.minimize(fun, weights, jac=jac, method='CG', args=(X, y) + args, options={'maxiter': options['mb_maxiter']})\n",
      "        weights[:] = ans.x\n",
      "        \n",
      "    @staticmethod\n",
      "    def LBFGSB(fun, weights, jac, X, y, options, args=()):\n",
      "        ans = optimize.minimize(fun, weights, jac=jac, method='L-BFGS-B', args=(X, y) + args, options={'maxiter': options['mb_maxiter']})\n",
      "        weights[:] = ans.x\n",
      "    \n",
      "    @staticmethod\n",
      "    def minimize(fun, weights, jac, X, y, method, batch_size=50, tol=1e-6, maxiter=100, args=None, \n",
      "                 verbose=False, options=None, random_state=None, callback=None):\n",
      "        if method == 'GD':\n",
      "            assert 'learning_rate' in options, 'GD needs a learning rate'\n",
      "            if 'learning_rate_decay' not in options:\n",
      "                options['learning_rate_decay'] = 1\n",
      "            if 'momentum' in options:\n",
      "                if 'momemtum_decay' not in options:\n",
      "                    options['momemtum_decay'] = 1\n",
      "                options['step'] = 0\n",
      "                update = MinibatchOpti.GD_momentum\n",
      "            else:\n",
      "                update = MinibatchOpti.GD\n",
      "        elif method == 'RMSPROP':\n",
      "            options['moving_mean_squared'] = 1\n",
      "            update = MinibatchOpti.RMSPROP\n",
      "        elif method == 'CG':\n",
      "            update = MinibatchOpti.CG\n",
      "        elif method == 'L-BFGS-B':\n",
      "            update = MinibatchOpti.LBFGSB\n",
      "        else:\n",
      "            raise Exception('Optimization method not found')\n",
      "\n",
      "        i = 1\n",
      "        prev_cost = 1e8\n",
      "        for _X, _y in MinibatchOpti.minibatches(X, y, batch_size, random_state=random_state):\n",
      "            update(fun, weights, jac, _X, _y, options, args=args)\n",
      "            new_cost = fun(weights, X, y, *args)\n",
      "            diff = new_cost - prev_cost\n",
      "            if np.abs(diff) < tol:\n",
      "                if verbose >= 1:\n",
      "                    print 'Minimum tolerance reached in %i iterations' % i\n",
      "                break\n",
      "            if i >= maxiter:\n",
      "                if verbose >= 1 :\n",
      "                    print 'Maximum number of iterations reached'\n",
      "                break\n",
      "            if verbose >= 2:\n",
      "                print i, new_cost    \n",
      "            if callback is not None:\n",
      "                stop = callback(i, weights)\n",
      "                if stop == True:\n",
      "                    break\n",
      "            prev_cost = new_cost\n",
      "            i = i + 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Class"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NN(object):\n",
      "    \n",
      "    def __init__(self, hidden_layers, coef0=None, random_state=None,\n",
      "                 opti_method='GD', batch_size=50, maxiter=100, tol=1e-6, verbose=1, \n",
      "                 opti_options=None, callback=None):\n",
      "        self.hidden_layers = hidden_layers\n",
      "        self.coef_ = None if coef0 is None else np.copy(coef0)\n",
      "        \n",
      "        if random_state is None:\n",
      "            self.rnd = np.random.RandomState()\n",
      "        elif isinstance(random_state, int):\n",
      "            self.rnd = np.random.RandomState(random_state)\n",
      "        else:\n",
      "            self.rnd = random_state\n",
      "        \n",
      "        self.opti_method = opti_method\n",
      "        self.batch_size = batch_size\n",
      "        self.verbose = verbose\n",
      "        self.maxiter = maxiter\n",
      "        self.tol = tol\n",
      "        self.opti_options = {} if opti_options is None else opti_options\n",
      "        self.callback = callback\n",
      "        \n",
      "    def rand_init(self, weights_info, random_state):\n",
      "        w_sizes = []\n",
      "        for layer_info in weights_info:\n",
      "            w_sizes.append(layer_info[0] * layer_info[1])\n",
      "        ans = np.zeros(sum(w_sizes))\n",
      "        start_pos = 0\n",
      "        for i, layer in enumerate(weights_info):\n",
      "            end_pos = start_pos + layer[0] * (layer[1])\n",
      "            gap = 4 * np.sqrt(6. / (layer[0] + layer[1]))\n",
      "            ans[start_pos:end_pos] = random_state.uniform(low=-gap, high=gap, size=w_sizes[i])\n",
      "            start_pos = end_pos \n",
      "        return ans\n",
      "    \n",
      "    def predict_proba(self, X):\n",
      "        output = self.layers[0].output(X)\n",
      "        for layer in self.layers[1:]:\n",
      "            output = layer.output(output)\n",
      "        return output\n",
      "   \n",
      "    def predict(self, X):\n",
      "        return self.predict_proba(X).argmax(1)\n",
      "    \n",
      "    def fit(self, X, y):\n",
      "        layers = list(self.hidden_layers)  # Copy\n",
      "        layers.insert(0, X.shape[1])\n",
      "        layers.insert(len(layers), len(np.unique(y)))\n",
      "        self.weights_info = [(layers[i] + 1, layers[i + 1]) for i in range(len(layers) - 1)]\n",
      "        self.opti_options = self.opti_options.copy()\n",
      "        \n",
      "        if self.coef_ is None:\n",
      "            self.coef_ = self.rand_init(self.weights_info, self.rnd)\n",
      "\n",
      "        # Unpack the weights and assign them to the layers\n",
      "        self.layers = []\n",
      "        start_pos = 0\n",
      "        for w_info in self.weights_info:\n",
      "            end_pos = start_pos + w_info[0] * (w_info[1])\n",
      "            weight = self.coef_[start_pos:end_pos].reshape((w_info[0], w_info[1]))\n",
      "            self.layers.append(SigmoidLayer(W=weight))\n",
      "            start_pos = end_pos\n",
      "        \n",
      "        args = (self.weights_info, len(np.unique(y)))\n",
      "        MinibatchOpti.minimize(cost, self.coef_, cost_prime, X, y, method=self.opti_method,\n",
      "                               random_state=self.rnd, batch_size=self.batch_size, maxiter=self.maxiter, \n",
      "                               tol=self.tol, args=args, verbose=self.verbose, options=self.opti_options,\n",
      "                               callback=self.callback)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "options = {}\n",
      "options['mb_maxiter'] = 10"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nn = NN([25], verbose=2, opti_method='CG', maxiter=100, batch_size=100, opti_options=options, random_state=1234)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nn.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[13830 25149 39551 ..., 18808 45746 24815]\n",
        "1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.23870590696\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.19459077781\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.16554769787\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.26125635937\n",
        "5"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-20-e493766e0c00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-12-795dd74779b4>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxiter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                                \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopti_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                                callback=self.callback)\n\u001b[0m",
        "\u001b[0;32m<ipython-input-11-3b8cc61a8781>\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, weights, jac, X, y, method, batch_size, tol, maxiter, args, verbose, options, random_state, callback)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_y\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mMinibatchOpti\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminibatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mnew_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_cost\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprev_cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-8-2a58dee547a4>\u001b[0m in \u001b[0;36mcost\u001b[0;34m(weights, X, y, weights_meta, num_labels)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mact_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munpack_weigths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/danielfrg/envs/copper/lib/python2.7/site-packages/numpy/lib/function_base.pyc\u001b[0m in \u001b[0;36minsert\u001b[0;34m(arr, obj, values, axis)\u001b[0m\n\u001b[1;32m   3625\u001b[0m     \u001b[0mslobj2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3626\u001b[0m     \u001b[0mnew\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslobj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3627\u001b[0;31m     \u001b[0mnew\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslobj2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3629\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.15580368423\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(nn.predict(X_valid) == y_valid).mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "0.85340000000000005"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nn = NN([25], verbose=2, opti_method='CG', maxiter=30, batch_size=100, opti_options=options, coef0=nn.coef_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cPickle, gzip, numpy\n",
      "f = gzip.open('mnist.pkl.gz', 'rb')\n",
      "train_set, valid_set, test_set = cPickle.load(f)\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, y_train = train_set[0], train_set[1]\n",
      "X_valid, y_valid = valid_set[0], valid_set[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def validate(epoch, weights):\n",
      "    if (nn.predict(X) == y).mean() > 0.50:\n",
      "        return True"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 81
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import datasets\n",
      "iris = datasets.load_iris()\n",
      "X = iris.data\n",
      "y = iris.target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}